{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Monte Carlo Control without Exploring Starts \n",
    "\n",
    "**Question.** How can we avoid the unlikely assumption of exploring starts? (That all actions are selected infinitely often)\n",
    "\n",
    "**Solutions.** On-policy methods and off-policy methods.... \n",
    "\n",
    "### On-Policy Methods\n",
    "On-policy methods attempt to evaluate or to improve the policy that is being used to make decisions, whereas off-policy methods evaluate or improve a policy different from the one used to generate the data; with Monte Carlo methods the data here are the episodes generated by the policy. The Monte Carlo ES method above is considered a on-policy method, but we now discuss a on-policy method  which does not assume anything about exploring starts. Off-policy methods are considered in the next section. \n",
    "\n",
    "* On-policy control methods typically have *soft policies*, meaning that $\\pi(a, s) > 0$ for all $s \\in \\mathcal{S}$ and all $a \\in \\mathcal{A}(s)$. \n",
    "\n",
    "The most popular soft policies are the *$\\epsilon$-greedy* policies, meaning that most of the time they choose an action that has a maximal estimated action value, but with probability $\\epsilon$ they instead select an action at random (recall such policies from the Bandits problem covered in Sutton & Barto Chapter 2). \n",
    "\n",
    "* ($\\implies$) All nongreedy actions are given the minimal probability of selection $\\frac{\\epsilon}{|\\mathcal{A}(s)|}$. \n",
    "\n",
    "* ($\\implies$) The remaining greedy action is given the remaining buld probability of $1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|}$. \n",
    "\n",
    "Find the code implementing **on-policy first-visit Monte Carlo control (for $\\epsilon$-soft policies), estimates for $\\pi \\approx \\pi_{*}$** in the following code cell.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def first_visit_monte_carlo_es(env, gamma=1.0, num_episodes=10_000):\n",
    "\n",
    "    # We will use Python dictionaries to represent the policy, Q and returns.\n",
    "    # The policy is a mapping from state to action; initially, it is a random policy.\n",
    "    pi = {state : np.random.choice([action for action in env.action_space]) for state in env.state_space}\n",
    "\n",
    "    # Q is a mapping from state-action pair to expected return.\n",
    "    Q = {state : {action : 0.0 for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    # returns is a mapping from state-action pair to a list of returns.\n",
    "    returns = {state : {action : [] for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        # Generate an episode.\n",
    "        episode = []\n",
    "\n",
    "        # Starting from a random state and action, after which the policy is followed.\n",
    "        state = env.state_space.sample()\n",
    "        action = env.action_space.sample()\n",
    "        env.reset(observation=state)\n",
    "        next_state, reward, terminated = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "        # Follow the policy.\n",
    "        while not terminated:\n",
    "            action = pi[state]\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Update Q and pi. This is the first-visit MC method.\n",
    "        # G is the expected return from the current state onwards.\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma*G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "                pi[state] = np.argmax(Q[state])\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom blackjack environment\n",
    "from custom_classes import CustomBlackjackEnv\n",
    "env = CustomBlackjackEnv()\n",
    "\n",
    "pi = first_visit_monte_carlo_es(env, num_episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random policy win percentage over 100000 games: 28.4%\n",
      "Policy exploring starts win percentage over 100000 games: 38.5%\n"
     ]
    }
   ],
   "source": [
    "num_games = 100_000\n",
    "\n",
    "################################################################################\n",
    "# Random policy win percentage over 100_000 games.\n",
    "\n",
    "num_wins = 0\n",
    "for _ in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated = env.step(action)\n",
    "    if reward == 1:\n",
    "        num_wins += 1\n",
    "win_percentage = num_wins / num_games * 100\n",
    "print(f\"Random policy win percentage over {num_games} games: {win_percentage:.1f}%\")\n",
    "\n",
    "################################################################################\n",
    "# Learned policy win percentage over 100_000 games.\n",
    "num_wins = 0\n",
    "for _ in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = pi[observation]\n",
    "        observation, reward, terminated = env.step(action)\n",
    "    if reward == 1:\n",
    "        num_wins += 1\n",
    "win_percentage = num_wins / num_games * 100\n",
    "print(f\"Policy exploring starts win percentage over {num_games} games: {win_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_monte_carlo_control(env, gamma=1.0, epsilon=0.1, num_episodes=10_000):\n",
    "\n",
    "    # We will use Python dictionaries to represent the policy, Q and returns.\n",
    "    # epsilon-soft policy.\n",
    "    pi = {state : np.ones(env.action_space.n)/env.action_space.n for state in env.state_space}\n",
    "\n",
    "    # Q is a mapping from state-action pair to expected return.\n",
    "    Q = {state : {action : 0.0 for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    # returns is a mapping from state-action pair to a list of returns.\n",
    "    returns = {state : {action : [] for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    action_space = [a for a in env.action_space]\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        # Generate an episode.\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = np.random.choice(action_space, p=pi[state])\n",
    "        next_state, reward, terminated = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        while not terminated:\n",
    "            action = np.random.choice(action_space, p=pi[state])\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma*G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "                A_star = np.argmax(Q[state])\n",
    "                for a in env.action_space:\n",
    "                    pi[state][a] = 1 - epsilon + epsilon / env.action_space.n if a == A_star else epsilon / env.action_space.n\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = on_policy_first_visit_monte_carlo_control(env,epsilon=0.0025, num_episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage over 100000 games: 38.2%\n"
     ]
    }
   ],
   "source": [
    "# Learned policy win percentage over 100_000 games.\n",
    "num_games = 100_000\n",
    "num_wins = 0\n",
    "for _ in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = np.random.choice([a for a in env.action_space], p=pi[observation])\n",
    "        observation, reward, terminated = env.step(action)\n",
    "    if reward == 1:\n",
    "        num_wins += 1\n",
    "\n",
    "win_percentage = num_wins / num_games * 100\n",
    "print(f\"Win percentage over {num_games} games: {win_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
