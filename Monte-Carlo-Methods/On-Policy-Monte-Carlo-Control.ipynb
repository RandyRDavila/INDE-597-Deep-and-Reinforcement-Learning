{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Monte Carlo Control (On-Policy)\n",
    "Monte Carlo Control is a method used in reinforcement learning to find the optimal policy for decision-making problems. Unlike policy evaluation, which estimates the value of a given policy, Monte Carlo Control focuses on improving the policy itself based on the returns received from episodes. This method does not require a model of the environment and uses the concept of exploration and exploitation to balance between trying out new actions and sticking with the best-known actions.\n",
    "\n",
    "## Why use $Q(s,a)$ instead of $V(s)$ in Monte Carlo Methods?\n",
    "\n",
    "1. Policy imporvment and optimization: The fundamental goal of reinforcement learning is to find an optimal policy. While $V(s)$ provides the value of being in a state under a certain policy, it does not directly inform the agent about which action to take. $Q(s,a)$, on the other hand, directly evaluates the potential of each action, making it more straightforward to select the best action without needing a model of the environment. This is particularly useful in Monte Carlo Control methods for policy improvement, where the policy is made greedy with respect to the $Q$ values.\n",
    "\n",
    "\n",
    "2. Dealing with model-free environments: Moreover, Monte Carlo methods are model-free, meaning they do not require knowledge of the environment's dynamics (i.e., the transition probabilities and rewards). While both $V$ and $Q$ functions can be estimated without a model, the $Q$ function is more conducive to model-free control. This is because once $Q(s,a)$ is known, the agent can directly decide the best action without needing to know the transition probabilities to the next states, which would be required to use $V(s)$ effectively for control.\n",
    "\n",
    "<img src=\"images/mc-es-first-visit.png\" width=\"1000\" height=\"580\" >\n",
    "\n",
    "In the following code cell we implement this algorithm. \n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def first_visit_monte_carlo_es(env, gamma=1.0, num_episodes=10_000):\n",
    "\n",
    "    # We will use Python dictionaries to represent the policy, Q and returns.\n",
    "    # The policy is a mapping from state to action; initially, it is a random policy.\n",
    "    pi = {state : np.random.choice([action for action in env.action_space]) for state in env.state_space}\n",
    "\n",
    "    # Q is a mapping from state-action pair to expected return.\n",
    "    Q = {state : {action : 0.0 for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    # returns is a mapping from state-action pair to a list of returns.\n",
    "    returns = {state : {action : [] for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        # Generate an episode.\n",
    "        episode = []\n",
    "\n",
    "        # Starting from a random state and action, after which the policy is followed.\n",
    "        state = env.state_space.sample()\n",
    "        action = env.action_space.sample()\n",
    "        env.reset(observation=state)\n",
    "        next_state, reward, terminated = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "        # Follow the policy.\n",
    "        while not terminated:\n",
    "            action = pi[state]\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Update Q and pi. This is the first-visit MC method.\n",
    "        # G is the expected return from the current state onwards.\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma*G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "                pi[state] = np.argmax(Q[state])\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom blackjack environment\n",
    "from custom_classes import CustomBlackjackEnv\n",
    "env = CustomBlackjackEnv()\n",
    "\n",
    "pi = first_visit_monte_carlo_es(env, num_episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random policy win percentage over 100000 games: 28.1%\n",
      "Policy exploring starts win percentage over 100000 games: 38.5%\n"
     ]
    }
   ],
   "source": [
    "from mc_utils import win_percentage\n",
    "num_games = 100_000\n",
    "\n",
    "################################################################################\n",
    "# Random policy win percentage over 100_000 games.\n",
    "\n",
    "num_wins = 0\n",
    "for _ in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated = env.step(action)\n",
    "    if reward == 1:\n",
    "        num_wins += 1\n",
    "win_percentage = num_wins / num_games * 100\n",
    "print(f\"Random policy win percentage over {num_games} games: {win_percentage:.1f}%\")\n",
    "\n",
    "################################################################################\n",
    "# Learned policy win percentage over 100_000 games.\n",
    "num_wins = 0\n",
    "for _ in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = pi[observation]\n",
    "        observation, reward, terminated = env.step(action)\n",
    "    if reward == 1:\n",
    "        num_wins += 1\n",
    "win_percentage = num_wins / num_games * 100\n",
    "print(f\"Policy exploring starts win percentage over {num_games} games: {win_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_monte_carlo_control(env, gamma=1.0, epsilon=0.1, num_episodes=10_000):\n",
    "\n",
    "    # We will use Python dictionaries to represent the policy, Q and returns.\n",
    "    # epsilon-soft policy.\n",
    "    pi = {state : np.ones(env.action_space.n)/env.action_space.n for state in env.state_space}\n",
    "\n",
    "    # Q is a mapping from state-action pair to expected return.\n",
    "    Q = {state : {action : 0.0 for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    # returns is a mapping from state-action pair to a list of returns.\n",
    "    returns = {state : {action : [] for action in env.action_space} for state in env.state_space}\n",
    "\n",
    "    action_space = [a for a in env.action_space]\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        # Generate an episode.\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = np.random.choice(action_space, p=pi[state])\n",
    "        next_state, reward, terminated = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        while not terminated:\n",
    "            action = np.random.choice(action_space, p=pi[state])\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma*G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "                A_star = np.argmax(Q[state])\n",
    "                for a in env.action_space:\n",
    "                    pi[state][a] = 1 - epsilon + epsilon / env.action_space.n if a == A_star else epsilon / env.action_space.n\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = on_policy_first_visit_monte_carlo_control(env,epsilon=0.0025, num_episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage over 100000 games: 38.2%\n"
     ]
    }
   ],
   "source": [
    "# Learned policy win percentage over 100_000 games.\n",
    "num_games = 100_000\n",
    "num_wins = 0\n",
    "for _ in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = np.random.choice([a for a in env.action_space], p=pi[observation])\n",
    "        observation, reward, terminated = env.step(action)\n",
    "    if reward == 1:\n",
    "        num_wins += 1\n",
    "\n",
    "win_percentage = num_wins / num_games * 100\n",
    "print(f\"Win percentage over {num_games} games: {win_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
