{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Dynamic Programming \n",
    "Given complete knowledge of a model of the environment as a finite Markov decision problem, we may use algorithms from dynamic programming to compute optimal policies. More specifically, assuming we know the dynamics of the MDP, in this notebook we explore the following algorithms:\n",
    "\n",
    "* Iterative Policy Evalution, for estimating $V \\approx v_{\\pi}$\n",
    "* Policy Improvment, for estimating $\\pi \\approx \\pi_*$\n",
    "\n",
    "The environment for which we will study these algorithms will be the *grid world* environment with deterministic dynamics. In the following code cell we define our `GridWorld` Python `class` and a helper `Actions` Python `class` object. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "class Actions(Enum):\n",
    "    \"\"\"\n",
    "    Enumeration of possible actions in the GridWorld environment.\n",
    "\n",
    "    Attributes:\n",
    "        UP (int): Represents the action of moving up.\n",
    "        RIGHT (int): Represents the action of moving right.\n",
    "        DOWN (int): Represents the action of moving down.\n",
    "        LEFT (int): Represents the action of moving left.\n",
    "    \"\"\"\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A simple grid world environment for reinforcement learning.\n",
    "\n",
    "    Attributes:\n",
    "        rows (int): The number of rows in the grid (rows x cols).\n",
    "        cols (int): The number of cols in the grid (rows x cols).\n",
    "\n",
    "    Methods:\n",
    "        is_terminal(state): Checks if a given state is terminal.\n",
    "        step(state, action): Takes a step in the environment given a state and\n",
    "        an action.\n",
    "        render(policy=None, agent=None): Renders the grid world environment. If\n",
    "        a policy is given, the policy will be displayed on the grid. If an agent\n",
    "        is given, the agent's position will be displayed on the grid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows=4, cols=4):\n",
    "        \"\"\"\n",
    "        Initializes the GridWorld environment.\n",
    "\n",
    "        Parameters:\n",
    "            size (int): The size of the grid world (default is 4x4).\n",
    "        \"\"\"\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.move = {\n",
    "            Actions.LEFT: np.array([0, -1]),\n",
    "            Actions.RIGHT: np.array([0, 1]),\n",
    "            Actions.UP: np.array([-1, 0]),\n",
    "            Actions.DOWN: np.array([1, 0]),\n",
    "        }\n",
    "        self.state_space = [(i, j) for i in range(self.rows) for j in range(self.cols)]\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Checks if the given state is a terminal state.\n",
    "\n",
    "        Parameters:\n",
    "            state (tuple): The state to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the state is terminal, False otherwise.\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        return (x == 0 and y == 0) or (x == self.rows - 1 and y == self.cols - 1)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        Performs an action in the environment from a given state.\n",
    "\n",
    "        Parameters:\n",
    "            state (tuple): The current state.\n",
    "            action (Actions): The action to be performed.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The next state and the reward received.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        next_state = (state + self.move[action]).tolist()\n",
    "        x, y = next_state\n",
    "\n",
    "        # Check for boundaries of the grid.\n",
    "        if x < 0 or x >= self.rows or y < 0 or y >= self.cols:\n",
    "            next_state = state.tolist()\n",
    "\n",
    "        reward = -1\n",
    "        done = self.is_terminal(next_state)\n",
    "        return tuple(next_state), reward, done\n",
    "\n",
    "    # Render the grid for visualizations of the agents progress.\n",
    "    def render(self, policy=None, agent=None):\n",
    "        \"\"\"\n",
    "        Renders the grid world environment.\n",
    "\n",
    "        Parameters:\n",
    "            policy (function): A function that takes a state and returns an\n",
    "            action. If given, the policy will be displayed on the grid.\n",
    "            agent (tuple): The position of the agent. If given, the agent's\n",
    "            position will be displayed on the grid.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Top border of the grid\n",
    "        print('+' + '---+' * self.cols)\n",
    "\n",
    "        for i in range(self.rows):\n",
    "            print('|', end='')\n",
    "            for j in range(self.cols):\n",
    "                cell = ' '\n",
    "                if agent and (i, j) == agent:\n",
    "                    # Agent's position.\n",
    "                    cell = 'A'\n",
    "                elif self.is_terminal((i, j)):\n",
    "                    # Hole in red.\n",
    "                    cell = '\\033[91mT\\033[0m'\n",
    "                elif policy:\n",
    "                    # Display policy direction.\n",
    "                    action = policy((i, j))\n",
    "                    if action == Actions.UP:\n",
    "                        cell = '↑'\n",
    "                    elif action == Actions.RIGHT:\n",
    "                        cell = '→'\n",
    "                    elif action == Actions.DOWN:\n",
    "                        cell = '↓'\n",
    "                    elif action == Actions.LEFT:\n",
    "                        cell = '←'\n",
    "\n",
    "                print(f' {cell} |', end='')\n",
    "            # Bottom border of each row.\n",
    "            print('\\n+' + '---+' * self.cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Now that we have created our action and environment class, it is useful to explore the methods of the classes. Our environment class should be defined in such a way that we observe a state, take an action, and recieve numerical reward, and a next state. Start be instantiating the `GridWorld` environment, definining the agent state, and rendering the agent in the environment in the next code cell. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an instance of the GridWorld environment.\n",
    "env = GridWorld()\n",
    "\n",
    "# Example starting state.\n",
    "state = (0, 1)\n",
    "\n",
    "# Show the agent in the grid.\n",
    "env.render(agent=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Next choose an arbitrary action, say `action = Actions.RIGHT`, and ask the environment to take this action given the previous state. The environment should return to you a triple, namely, `(next_state, reward, done)`. As you can probably expect, `next_state` is the resulting state from taking `action` in the variable `state`; `done` is a boolean value indicating whether or not the agent has landed in a *terminal state*. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to the right.\n",
    "action = Actions.RIGHT\n",
    "\n",
    "# Step in the environment.\n",
    "next_state, reward, done = env.step(state, Actions.RIGHT)\n",
    "\n",
    "# Print the state, action, reward, next state tuple.\n",
    "print(f\"{state = }, {action = }, {reward = }, {next_state = }, {done = }\")\n",
    "\n",
    "env.render(agent=next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "In order to understand how we can solve MDP's with dynamic programming, we will first focus on random equaprobability policies. In the following code cell we write functions representing this notion. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi(s) = s'\n",
    "def random_policy(state):\n",
    "    return np.random.choice(list(Actions))\n",
    "\n",
    "# pi(a | s)\n",
    "# For our random policy, all actions are assumed to have the same probability\n",
    "# given state s.\n",
    "def random_policy_probability(state, action):\n",
    "    return 1/4\n",
    "\n",
    "# Show the actions made by the policy in the environment.\n",
    "env.render(policy=random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Iterative Policy Evaluation, for evaluting $V \\approx v_{\\pi}$\n",
    "\n",
    "In reinforcement learning, we seek good policies. Our goal here will be to use value functions to organize and structure the search for good policies. Before doing so, first recall the **Bellman optimality equations**:\n",
    "\n",
    "State Value Optimality: $v_*(s) = \\max_{a}\\sum_{s', r}p(s', r | s, a)[r + \\gamma v_*(s')]$\n",
    "\n",
    "State-Action Value Optimality: $q_*(s, a) = \\sum_{s', r}p(s', r | s, a)[r + \\gamma \\max_{a'}q_*(s', a')]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(\n",
    "        env,\n",
    "        policy_probability,\n",
    "        policy=None,\n",
    "        theta=1e-4,\n",
    "        show_iterations=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Iteratively evaluates a random policy in the given GridWorld environment.\n",
    "\n",
    "    Parameters:\n",
    "        env (GridWorld): The grid world environment.\n",
    "        theta (float): A threshold for the evaluation accuracy.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The final value function and the number of iterations.\n",
    "    \"\"\"\n",
    "    # Initialize value function to zeros.\n",
    "    V = np.zeros((env.rows, env.cols))\n",
    "\n",
    "    # Copy value function for synchronous updating of the values in V.\n",
    "    new_v = V.copy()\n",
    "\n",
    "    # Iteration count for tracking progress.\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        for state in env.state_space:\n",
    "            # Ignore the terminal states.\n",
    "            if env.is_terminal(state):\n",
    "                continue\n",
    "            else:\n",
    "                i, j = state\n",
    "            # Initialize value to sum onto over all actions given the state.\n",
    "            value = 0\n",
    "            for action in Actions:\n",
    "                # Calculate the value for each action.\n",
    "                next_state, reward, _ = env.step(state, action)\n",
    "                new_i, new_j = next_state\n",
    "                value += policy_probability(state, action) * (reward + V[new_i, new_j])\n",
    "\n",
    "            new_v[i, j] = value\n",
    "\n",
    "        # Check for convergence.\n",
    "        if np.sum(np.abs(new_v - V)) < theta:\n",
    "            V = new_v.copy()\n",
    "            break\n",
    "\n",
    "        V = new_v.copy()\n",
    "        iteration += 1\n",
    "\n",
    "    if show_iterations:\n",
    "        print(f\"The number of iterations = {iteration}\")\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = iterative_policy_evaluation(env, random_policy_probability)\n",
    "print(f\"{V = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table\n",
    "import matplotlib\n",
    "\n",
    "# Use the 'Agg' backend for matplotlib to avoid the need for a GUI.\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "def draw_image(env, values, fig_name='value-states'):\n",
    "    \"\"\"\n",
    "    Draws the grid world with values in each state.\n",
    "\n",
    "    Parameters:\n",
    "        values (np.array): Array containing the values for each state.\n",
    "        fig_name (str): The filename for saving the figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "    values = np.round(values, decimals=2)\n",
    "\n",
    "    nrows, ncols = env.rows, env.cols\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Adding cells to the table.\n",
    "    for (i, j), val in np.ndenumerate(values):\n",
    "        color = 'white'\n",
    "        tb.add_cell(i, j, width, height, text=val,\n",
    "                    loc='center', facecolor=color)\n",
    "\n",
    "    # Adding row and column labels.\n",
    "    for i in range(len(values)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "    for j in range(len(values)):\n",
    "        tb.add_cell(-1, j, width, height/2, text=j+1, loc='center',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "\n",
    "    ax.add_table(tb)\n",
    "    plt.savefig(f'{fig_name}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the value of states in a grid. This will be saved in the folder during\n",
    "# the colab session.\n",
    "draw_image(env, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a better policy based off of the values of states found by the value\n",
    "# iteration algorithm on the random policy.\n",
    "def better_policy(env, state, value_grid):\n",
    "    best_action = Actions.UP\n",
    "    next_state, reward, _ = env.step(state, best_action)\n",
    "    i, j = next_state\n",
    "    max_value = value_grid[i, j]\n",
    "    for action in [Actions.DOWN, Actions.LEFT, Actions.RIGHT]:\n",
    "        next_state, reward, _ = env.step(state, action)\n",
    "        i, j = next_state\n",
    "        if max_value < value_grid[i, j]:\n",
    "            best_action = action\n",
    "            max_value = value_grid[i, j]\n",
    "    return best_action\n",
    "\n",
    "# Specific instance of this.\n",
    "def pi(state):\n",
    "    return better_policy(env, state, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the new policy action for each state in the environment (excluding the\n",
    "# terminal states.\n",
    "for state in env.state_space:\n",
    "    if env.is_terminal(state):\n",
    "        continue\n",
    "    action = pi(state)\n",
    "    print(f\"{state=}, {action=} \\n\")\n",
    "\n",
    "print(f\"{V=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(policy=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gridworld_walk(env, starting_state, policy, value_table):\n",
    "    states = [starting_state]\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    time = []\n",
    "    for i in range(10):\n",
    "        action = policy(env, states[-1], value_table)\n",
    "        next_state, reward, done = env.step(states[-1], action)\n",
    "        time.append(i)\n",
    "        states.append(next_state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    for t, state, action, reward in zip(time, states, actions, rewards):\n",
    "        print(f\"----------------------------------\")\n",
    "        print(f\"Time: {t = } \")\n",
    "        print(f\"S_{t} = {state}, A_{t} = {action.name}, R_{t+1} = {reward} \\n\")\n",
    "        env.render(agent=tuple(state))\n",
    "        print()\n",
    "    print(f\"----------------------------------\")\n",
    "    print(\"Terminal State Reached \\n\")\n",
    "    env.render(agent=states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an agent following the better policy in the GridWorld environment.\n",
    "simulate_gridworld_walk(env, (2, 1), better_policy, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
