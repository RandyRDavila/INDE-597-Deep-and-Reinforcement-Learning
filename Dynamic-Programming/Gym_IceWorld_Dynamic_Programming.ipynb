{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "4i5FfPiRe5WG",
        "outputId": "ccc3ae03-0d73-4e98-ec16-34375102c94e"
      },
      "outputs": [],
      "source": [
        "# Install the gymnasium package and its dependencies for this notebook.\n",
        "# !pip install gymnasium\n",
        "# !pip install gymnasium[toy-text]\n",
        "# !pip install pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Instantiate the Frozen Lake environment. Set is_slippery=False for\n",
        "# deterministic environment dynamics.\n",
        "env =gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Print the action and observation spaces.\n",
        "print(f\"{env.action_space = }\")\n",
        "print(f\"{env.observation_space = } \\n\")\n",
        "\n",
        "# Show the numerical number of actions and spaces.\n",
        "print(f\"{env.action_space.n = }\")\n",
        "print(f\"{env.observation_space.n = } \\n\")\n",
        "\n",
        "# Always reset the environment before taking actions. This sets the\n",
        "# character in state 0 and returns a dictionary of information concerning\n",
        "# this initial state.\n",
        "print(f\"{env.reset() = } \\n \")\n",
        "\n",
        "# We can view the game state by calling plt.imshow on the rgb rendering of\n",
        "# the current game state.\n",
        "current_grid = env.render()\n",
        "plt.imshow(current_grid)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EmRTpnokDR4"
      },
      "source": [
        "---\n",
        "\n",
        "## Action Space\n",
        "\n",
        "The action shape is `(1,)` in the range `{0, 3}` indicating which direction to move the player.\n",
        "\n",
        "    0: Move left\n",
        "\n",
        "    1: Move down\n",
        "\n",
        "    2: Move right\n",
        "\n",
        "    3: Move up\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "M2z9VlPkgSrT",
        "outputId": "4da7c3ff-2726-44d7-a3de-50382ab8aa86"
      },
      "outputs": [],
      "source": [
        "state, reward, done, _, P = env.step(1) # Try and move down.\n",
        "print(f\"{state = }\")\n",
        "print(f\"{reward = }\")\n",
        "print(f\"{done = }\")\n",
        "print(f\"{_ = }\")\n",
        "print(f\"{P = }\")\n",
        "new_grid = env.render()\n",
        "plt.imshow(new_grid)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "tfgZJN3GiSdO",
        "outputId": "1e1b6bce-1bd3-40f9-e101-9a7dc4fb7679"
      },
      "outputs": [],
      "source": [
        "state, reward, done, _, P = env.step(1)\n",
        "print(f\"{state = }\")\n",
        "print(f\"{reward = }\")\n",
        "print(f\"{done = }\")\n",
        "print(f\"{_ = }\")\n",
        "print(f\"{P = }\")\n",
        "new_grid = env.render()\n",
        "plt.imshow(new_grid)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "pEnkZSrDjQb9",
        "outputId": "fce5e70d-d9b6-4b29-df27-3725260a5b9a"
      },
      "outputs": [],
      "source": [
        "state, reward, done, _, P = env.step(1)\n",
        "print(f\"{state = }\")\n",
        "print(f\"{reward = }\")\n",
        "print(f\"{done = }\")\n",
        "print(f\"{_ = }\")\n",
        "print(f\"{P = }\")\n",
        "new_grid = env.render()\n",
        "plt.imshow(new_grid)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "Dn4ek3mYkcHR",
        "outputId": "8c592f42-4731-4cdb-cf61-6de785b51c5c"
      },
      "outputs": [],
      "source": [
        "state, reward, done, _, P = env.step(2)\n",
        "print(f\"{state = }\")\n",
        "print(f\"{reward = }\")\n",
        "print(f\"{done = }\")\n",
        "print(f\"{_ = }\")\n",
        "print(f\"{P = }\")\n",
        "new_grid = env.render()\n",
        "plt.imshow(new_grid)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDe1TfOxkcRs",
        "outputId": "0c87a6a7-34f3-42b8-8816-4d0e386a165c"
      },
      "outputs": [],
      "source": [
        "state, P = env.reset()\n",
        "done = False\n",
        "grids = []\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, _, P = env.step(action)\n",
        "    new_grid = env.render()\n",
        "    grids.append(new_grid)\n",
        "\n",
        "\n",
        "len(grids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "V_vGDSsnkcVM",
        "outputId": "cbd22ddb-7747-4dc6-efef-4dd815363f22"
      },
      "outputs": [],
      "source": [
        "plt.imshow(grids[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "vQYC2mJGnjDw",
        "outputId": "f2677aef-002d-4e44-bfaf-0374955f5920"
      },
      "outputs": [],
      "source": [
        "plt.imshow(grids[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Ve8vH_OZnjJe",
        "outputId": "15cc15ec-b9dd-4cf7-fe31-858944a0f86f"
      },
      "outputs": [],
      "source": [
        "plt.imshow(grids[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIxlyHhrtYN3",
        "outputId": "95e3167e-2513-4f24-e2ad-29f6d4d26bd0"
      },
      "outputs": [],
      "source": [
        "# Show the transition probabilities for each state-action pair.\n",
        "print(f\"{env.unwrapped.P = }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOvFQ8LSO5r8"
      },
      "outputs": [],
      "source": [
        "# I need to fix this one.\n",
        "def run_episodes(environment, n_episodes, policy, display=True):\n",
        "    wins = 0\n",
        "    total_reward = 0\n",
        "    for episode in range(n_episodes):\n",
        "        terminated = False\n",
        "        state = environment.reset()\n",
        "        while not terminated:\n",
        "            # Select an action to perform in a current state\n",
        "            if isinstance(policy, str) and policy == 'random':\n",
        "                action = environment.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(policy[state])\n",
        "\n",
        "            # Perform an action and observe how environment acted in response\n",
        "            next_state, reward, terminated, info = environment.step(action)\n",
        "\n",
        "            # Plot the first episode\n",
        "            if episode==1 and display:\n",
        "                print(\"Action:\")\n",
        "                plt.imshow(environment.render()) # display current agent state\n",
        "            # Summarize total reward\n",
        "            total_reward += reward\n",
        "            # Update current state\n",
        "            state = next_state\n",
        "            # Calculate number of wins over episodes\n",
        "            if terminated and reward == 1.0:\n",
        "                wins += 1\n",
        "    average_reward = total_reward / n_episodes\n",
        "    return wins, total_reward, average_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk9AMpdotYQP"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy, environment, discount_factor=0.99, theta=1e-9, max_iterations=1e9):\n",
        "    # Number of evaluation iterations\n",
        "    evaluation_iterations = 1\n",
        "    # Initialize a value function for each state as zero\n",
        "    V = np.zeros(environment.observation_space.n)\n",
        "    # Repeat until change in value is below the threshold\n",
        "    for i in range(int(max_iterations)):\n",
        "        # Initialize a change of value function as zero\n",
        "        delta = 0\n",
        "        # Iterate though each state\n",
        "        for state in range(environment.observation_space.n):\n",
        "            v = V[state]\n",
        "            v_a = []\n",
        "            for action in range(environment.action_space.n):\n",
        "                v_x = 0\n",
        "                for (p, new_state, reward, terminating) in env.unwrapped.P[state][action]:\n",
        "                    v_x += policy[state][action] * p * (reward + V[new_state])\n",
        "                v_a.append(v_x)\n",
        "\n",
        "                # Update value function\n",
        "            V[state] = max(v_a)\n",
        "\n",
        "            # Calculate the absolute change of value function\n",
        "            delta = max(delta, np.abs(V[state] - v))\n",
        "            # Update value function\n",
        "            #V[state] = v\n",
        "        evaluation_iterations += 1\n",
        "\n",
        "        # Terminate if value change is insignificant\n",
        "        if delta < theta:\n",
        "            # TODO - check how many iterations\n",
        "            print(f'Policy-evaluation converged at iteration #{evaluation_iterations}')\n",
        "            return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuYr-UaqtYST",
        "outputId": "ee8bd1ca-de45-4097-e695-48f9677a9768"
      },
      "outputs": [],
      "source": [
        "pi = {state : {0 : .25, 1 : .25, 2 : .25, 3 : .25} for state in range(env.observation_space.n)}\n",
        "\n",
        "V = policy_evaluation(pi, env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVQeNqVNtYUR",
        "outputId": "47a84fbd-1753-4608-e18c-1c3f8b8ea7ea"
      },
      "outputs": [],
      "source": [
        "print(f\"{V = }\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH_sqMk1tYWW"
      },
      "outputs": [],
      "source": [
        "def one_step_lookahead(environment, state, V, discount_factor):\n",
        "    action_values = np.zeros(environment.action_space.n)\n",
        "    for action in range(environment.action_space.n):\n",
        "        for probability, next_state, reward, terminated in env.unwrapped.P[state][action]:\n",
        "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
        "    return action_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY_ccnDKy5tg",
        "outputId": "f9a37708-cb84-43b8-fa07-9d121bb6c813"
      },
      "outputs": [],
      "source": [
        "# Check that the function works!\n",
        "one_step_lookahead(env, 1, V, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aKDFrEdy5vu"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
        "    # Start with a random policy\n",
        "    # num states x num actions / num actions\n",
        "    policy = np.ones([environment.observation_space.n, environment.action_space.n]) / environment.action_space.n\n",
        "    # Initialize counter of evaluated policies\n",
        "    evaluated_policies = 1\n",
        "    # Repeat until convergence or critical number of iterations reached\n",
        "    for i in range(int(max_iterations)):\n",
        "        stable_policy = True\n",
        "        # Evaluate current policy\n",
        "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
        "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
        "        for state in range(environment.observation_space.n):\n",
        "            # Choose the best action in a current state under current policy\n",
        "            current_action =  policy[state].tolist().index(max(policy[state]))\n",
        "            # Look one step ahead and evaluate if current action is optimal\n",
        "            # We will try every possible action in a current state\n",
        "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            # Select a better action\n",
        "            best_action = action_value.tolist().index(max(action_value))\n",
        "            # If action didn't change\n",
        "            if current_action != best_action:\n",
        "                stable_policy = True\n",
        "                # Greedy policy update\n",
        "                policy[state] = np.eye(environment.action_space.n)[best_action]\n",
        "        evaluated_policies += 1\n",
        "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
        "        if stable_policy:\n",
        "            return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sd6KUpdy5yK",
        "outputId": "d1ee69ad-3e4c-418d-9295-abbfbd2ece4b"
      },
      "outputs": [],
      "source": [
        "policy, V = policy_iteration(env)\n",
        "print(f\"{policy = } \\n\")\n",
        "print(f\"{V = } \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHykDqbjN7Ri"
      },
      "outputs": [],
      "source": [
        "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "    # Initialize state-value function with zeros for each environment state\n",
        "    V = np.zeros(environment.observation_space.n)\n",
        "    for i in range(int(max_iterations)):\n",
        "        # Early stopping condition\n",
        "        delta = 0\n",
        "        # Update each state\n",
        "        for state in range(environment.observation_space.n):\n",
        "            # Do a one-step lookahead to calculate state-action values\n",
        "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            # Select best action to perform based on the highest state-action value\n",
        "            best_action_value = max(action_value)\n",
        "            # Calculate change in value\n",
        "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "            # Update the value function for current state\n",
        "            V[state] = best_action_value\n",
        "            # Check if we can stop\n",
        "        if delta < theta:\n",
        "            print(f'Value-iteration converged at iteration #{i}.')\n",
        "            break\n",
        "\n",
        "    # Create a deterministic policy using the optimal value function\n",
        "    policy = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
        "    for state in range(environment.observation_space.n):\n",
        "        # One step lookahead to find the best action for this state\n",
        "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "        # Select best action based on the highest state-action value\n",
        "        best_action = action_value.tolist().index(max(action_value))\n",
        "        # Update the policy to perform a better action at a current state\n",
        "        policy[state, best_action] = 1.0\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc1cG6DROTUF",
        "outputId": "b111a644-e05b-4237-a545-8cef50485b7d"
      },
      "outputs": [],
      "source": [
        "policy, V = value_iteration(env)\n",
        "print(f\"{policy = } \\n\")\n",
        "print(f\"{V = } \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0ShVQqJO_K-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
